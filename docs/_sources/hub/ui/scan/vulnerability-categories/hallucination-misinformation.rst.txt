:og:title: Giskard Hub UI - Hallucination & Misinformation Vulnerabilities (OWASP LLM 09)
:og:description: Comprehensive guide to hallucination and misinformation vulnerabilities in AI systems. Learn about RAGET testing, sycophancy detection, and strategies to prevent false information generation.

Hallucination & Misinformation (OWASP LLM 09)
=============================================

Probes for AI systems providing false, inconsistent, or fabricated information.

.. list-table::
   :header-rows: 1
   :widths: 25 75

   * - Probe Name
     - Description
   * - RAGET Complex Question
     - This probe tests for hallucination in Retrieval-Augmented Generation (RAG) systems by generating complex, verbose, and detailed questions based on documents in the provided knowledge base. Based on Giskard's RAG Evaluation Toolkit (RAGET)
   * - RAGET Simple Question 
     - This probe tests for hallucination in Retrieval-Augmented Generation (RAG) systems by generating simple, factual questions based on documents in the provided knowledge base and evaluating response accuracy. Based on Giskard's RAG Evaluation Toolkit (RAGET)
   * - RAGET Situational Question
     - This probe tests for hallucination in Retrieval-Augmented Generation (RAG) systems by generating questions that include situational context and user-specific scenarios based on documents in the provided knowledge base. Based on Giskard's RAG Evaluation Toolkit (RAGET)
   * - Sycophancy
     - This probe tests whether an AI agent exhibits sycophantic behavior by providing inconsistent or contradictory answers depending on the user's question framing