LLM Benchmarks
==============

LLM benchmarks are standardized tests designed to measure and compare the capabilities of different language models across various tasks and domains. These benchmarks provide a consistent framework for evaluating model performance, enabling researchers and practitioners to assess how well different LLMs handle specific challenges.

Types of LLM Benchmarks
-----------------------

.. grid:: 1 1 2 2

   .. grid-item-card:: Reasoning and Language Understanding
      :link: reasoning_and_language
      :link-type: doc

      Evaluations of logical inference, text comprehension, and language understanding.

   .. grid-item-card:: Math Problems
      :link: math_problems
      :link-type: doc

      Tasks from basic arithmetic to complex calculus and mathematical problem-solving.

   .. grid-item-card:: Coding
      :link: coding
      :link-type: doc

      Tests of code generation, debugging, and solving programming challenges.

   .. grid-item-card:: Conversation and Chatbot
      :link: conversation_and_chatbot
      :link-type: doc

      Assessments of dialogue engagement, context maintenance, and response helpfulness.

   .. grid-item-card:: Safety
      :link: safety
      :link-type: doc

      Evaluations of harmful content avoidance, bias detection, and manipulation resistance.

   .. grid-item-card:: Domain-Specific
      :link: domain_specific
      :link-type: doc

      Specialized benchmarks for fields like healthcare, finance, law, and medicine.

Creating your own evaluation benchmarks with Giskard
----------------------------------------------------

.. grid:: 1 1 2 2

   .. grid-item-card:: Giskard Hub AI security vulnerabilities evaluation
      :link: /hub/ui/datasets/security
      :link-type: doc

      Our state-of-the-art enterprise-grade security evaluation datasets.

   .. grid-item-card:: Giskard Hub AI business failures evaluation
      :link: /hub/ui/datasets/business
      :link-type: doc

      Our state-of-the-art enterprise-grade business failures evaluation datasets.

   .. grid-item-card:: LLM Scan
      :link: /oss/sdk/security
      :link-type: doc

      Our open-source library for creating security evaluation datasets.

   .. grid-item-card:: RAGET: RAG Evaluation Toolkit
      :link: /oss/sdk/business
      :link-type: doc

      Our open-source library for creating business evaluation datasets.

.. toctree::
   :maxdepth: 2
   :hidden:

   reasoning_and_language
   math_problems
   coding
   conversation_and_chatbot
   safety
   domain_specific
