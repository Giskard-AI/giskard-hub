=======================
Continuous red teaming
=======================

Red teaming involves simulating attacks to uncover an application's weaknesses and vulnerabilities. For LLMs, this process focuses on identifying risks such as hallucinations, misleading outputs, biased or discriminatory content, and more. At Giskard, we offer continuous red teaming to ensure your AI agent operates reliably throughout its lifecycle. This involves generating datasets from timely and relevant sources like news articles, Wikipedia, Trustpilot reviews, and others. By doing so, we minimize the risk of attacks and help stabilize the system over time.

Each new dataset is tested against your agent, and Giskard notifies you of any identified vulnerabilities, whether major or minor. We also provide key metrics to help you address these issues effectively.